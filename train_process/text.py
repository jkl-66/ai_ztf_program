import os
import json
import torch
from transformers.models.auto.modeling_auto import AutoModelForCausalLM
from transformers.models.auto.tokenization_auto import AutoTokenizer
from transformers.generation.configuration_utils import GenerationConfig
from peft import PeftModel
from huggingface_hub import login

# Hugging Faceè®¤è¯ï¼ˆä¸train.pyä¿æŒä¸€è‡´ï¼‰
login(token=" ")

# 1. åŠ è½½LoRAè®­ç»ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨
def load_trained_model(model_path: str, base_model_name: str = "Qwen/Qwen-7B-Chat"):
    """ä»æœ¬åœ°è·¯å¾„åŠ è½½LoRAè®­ç»ƒåçš„Qwenæ¨¡å‹"""
    try:
        print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
        # é¦–å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.bfloat16
        )
        
        print(f"åŠ è½½LoRAé€‚é…å™¨ä»: {model_path}")
        # åŠ è½½LoRAé€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("LoRAæ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise

# 2. æ„é€ Qwené—®ç­”æ ¼å¼çš„promptï¼ˆä¸train.pyä¿æŒä¸€è‡´ï¼‰
def build_qa_prompt(question: str, history: list | None = None) -> str:
    """
    æ„é€ ç¬¦åˆQwenå¯¹è¯æ ¼å¼çš„é—®ç­”prompt
    - history: å¤šè½®å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º[(user_question, assistant_answer)]
    """
    if history is None:
        history = []

    try:
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ ¼å¼
        prompt_parts = []
        for user_q, assistant_a in history:
            prompt_parts.append(f"<|im_start|>user\n{user_q}<|im_end|>")
            prompt_parts.append(f"<|im_start|>assistant\n{assistant_a}<|im_end|>")

        # æ·»åŠ å½“å‰é—®é¢˜
        prompt_parts.append(f"<|im_start|>user\n{question}<|im_end|>")
        prompt_parts.append(f"<|im_start|>assistant\n")

        return "".join(prompt_parts)
    except Exception as e:
        print(f"æ„é€ promptå¤±è´¥: {e}")
        raise

# 3. é—®ç­”ç”Ÿæˆå‡½æ•°ï¼ˆä¼˜åŒ–å‚æ•°é€‚é…LoRAæ¨¡å‹ï¼‰
def generate_answer(
    model, 
    tokenizer, 
    question: str, 
    history: list | None = None,
    max_new_tokens: int = 256,  # ä¸è®­ç»ƒæ—¶çš„max_lengthä¿æŒä¸€è‡´
    temperature: float = 0.7,
    top_p: float = 0.9,
    use_history: bool = True
):
    """
    ç”Ÿæˆé—®é¢˜å›ç­”
    - use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²ï¼ˆå¤šè½®é—®ç­”ï¼‰
    """
    try:
        # æ„é€ prompt
        prompt = build_qa_prompt(question, history if (use_history and history) else None)

        # é…ç½®ç”Ÿæˆå‚æ•°ï¼ˆä¸train.pyä¸­çš„generate_textå‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        generation_config = GenerationConfig(
            temperature=temperature,
            top_p=top_p,
            max_new_tokens=max_new_tokens,
            repetition_penalty=1.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                generation_config=generation_config
            )

        # è§£ç ç»“æœå¹¶æå–assistantå›ç­”éƒ¨åˆ†
        generated_ids = outputs[0][inputs.input_ids.shape[1]:]
        answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        # æ¸…ç†å›ç­”æ–‡æœ¬
        answer = answer.split("<|im_end|>")[0].strip()
        
        return answer
    except Exception as e:
        print(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
        raise

# 4. é—®ç­”äº¤äº’å‡½æ•°ï¼ˆæ”¯æŒå•è½®/å¤šè½®ï¼‰
def qa_interaction(model, tokenizer, use_mult_rounds: bool = False):
    """é—®ç­”äº¤äº’ä¸»å‡½æ•°"""
    print("=== Qwen LoRA å¾®è°ƒæ¨¡å‹é—®ç­”äº¤äº’ç³»ç»Ÿ ===")
    print("è¾“å…¥'é€€å‡º'ã€'q'æˆ–'quit'ç»“æŸäº¤äº’")
    print("è¾“å…¥'æ¸…ç©º'æˆ–'clear'æ¸…ç©ºå¯¹è¯å†å²")
    print(f"å¤šè½®å¯¹è¯æ¨¡å¼: {'å¼€å¯' if use_mult_rounds else 'å…³é—­'}")
    print("-" * 50)

    history: list[tuple[str, str]] = []

    while True:
        try:
            # è·å–ç”¨æˆ·é—®é¢˜
            question = input("\nğŸ¤” è¯·è¾“å…¥ä½ çš„é—®é¢˜: ")
            
            if question.lower() in ["é€€å‡º", "q", "quit"]:
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif question.lower() in ["æ¸…ç©º", "clear"]:
                history.clear()
                print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            elif not question.strip():
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                continue

            print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
            
            # ç”Ÿæˆå›ç­”
            answer = generate_answer(
                model, 
                tokenizer, 
                question, 
                history if use_mult_rounds else None,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9
            )

            # æ˜¾ç¤ºå›ç­”
            print(f"\nğŸ¤– æ¨¡å‹å›ç­”:")
            print(f"{answer}")

            # ä¿å­˜åˆ°å†å²ï¼ˆå¤šè½®æ¨¡å¼ï¼‰
            if use_mult_rounds:
                history.append((question, answer))
                print(f"\nğŸ“ å¯¹è¯è½®æ¬¡: {len(history)}")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ äº¤äº’è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è¾“å…¥'é€€å‡º'ç»“æŸç¨‹åº")

# 5. æ‰¹é‡æµ‹è¯•å‡½æ•°
def batch_test(model, tokenizer, test_questions: list[str]):
    """æ‰¹é‡æµ‹è¯•æ¨¡å‹å›ç­”è´¨é‡"""
    print("=== æ‰¹é‡æµ‹è¯•æ¨¡å¼ ===")
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- æµ‹è¯• {i}/{len(test_questions)} ---")
        print(f"é—®é¢˜: {question}")
        
        try:
            answer = generate_answer(model, tokenizer, question, max_new_tokens=200)
            print(f"å›ç­”: {answer}")
        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
        
        print("-" * 50)

# 6. ä¸»å‡½æ•°ï¼ˆæ•´åˆæ¨¡å‹åŠ è½½ä¸äº¤äº’ï¼‰
if __name__ == "__main__":
    try:
        # LoRAæ¨¡å‹è·¯å¾„ï¼ˆä¸train.pyä¸­çš„output_dirä¸€è‡´ï¼‰
        lora_model_path = "./qwen_7b_trained_with_prompt_lora"
        base_model_name = "Qwen/Qwen-7B-Chat"
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(lora_model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {lora_model_path}")
            print("è¯·ç¡®ä¿å·²å®Œæˆæ¨¡å‹è®­ç»ƒï¼Œæˆ–æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            exit(1)

        print("ğŸš€ å¼€å§‹åŠ è½½LoRAå¾®è°ƒæ¨¡å‹...")
        # åŠ è½½æ¨¡å‹
        model, tokenizer = load_trained_model(lora_model_path, base_model_name)
        
        print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # é€‰æ‹©äº¤äº’æ¨¡å¼
        print("\nè¯·é€‰æ‹©æ¨¡å¼:")
        print("1. å•è½®é—®ç­”ï¼ˆé»˜è®¤ï¼‰")
        print("2. å¤šè½®å¯¹è¯")
        print("3. æ‰¹é‡æµ‹è¯•")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
        
        if choice == "2":
            qa_interaction(model, tokenizer, use_mult_rounds=True)
        elif choice == "3":
            # é¢„è®¾æµ‹è¯•é—®é¢˜
            test_questions = [
                "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
                "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
                "æ¨¡ä»¿é²è¿…çš„ç¬”è§¦ï¼Œæè¿°ä¸€ä¸‹æ˜¥å¤©çš„æ™¯è‰²",
                "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "å†™ä¸€é¦–å…³äºå‹è°Šçš„è¯—"
            ]
            batch_test(model, tokenizer, test_questions)
        else:
            qa_interaction(model, tokenizer, use_mult_rounds=False)
            
    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")